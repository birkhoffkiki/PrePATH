from PIL import Image
import torch
from transformers import AutoImageProcessor, AutoModel
from torchvision import transforms
from PIL.Image import Resampling

# Load phikon-v2

def get_model(device):
    model = AutoModel.from_pretrained("owkin/phikon-v2").to(device)
    model.eval()
    
    def func(image):
        # æ£€æŸ¥è¾“å…¥ç±»å‹ï¼Œæ”¯æŒä¸¤ç§æ–¹å¼
        if isinstance(image, torch.Tensor):
            # å·²ç»é¢„å¤„ç†è¿‡çš„tensorï¼ˆæ¥è‡ªDataLoaderï¼‰
            if image.dim() == 3:  # (3, 224, 224)
                image = image.unsqueeze(0)  # æ·»åŠ batchç»´åº¦
            inputs = {"pixel_values": image.to(device, non_blocking=True)}
        else:
            # PILå›¾åƒï¼ˆå…¼å®¹åŸæœ‰æ–¹å¼ï¼Œä½†æ•ˆç‡è¾ƒä½ï¼‰
            processor = AutoImageProcessor.from_pretrained("owkin/phikon-v2")
            inputs = processor(image, return_tensors="pt")
            inputs = {k: v.to(device, non_blocking=True) for k, v in inputs.items()}
        
        # Get the features
        with torch.inference_mode():
            outputs = model(**inputs)
            features = outputs.last_hidden_state[:, 0, :]  # (1, 1024) shape
        return features
    
    return func

def get_phikon2_trans():
    """
    è·å–phikon2çš„é¢„å¤„ç†transformï¼Œå®Œå…¨ç­‰ä»·äºprocessor
    """
    processor = AutoImageProcessor.from_pretrained("owkin/phikon-v2")
    
    transform = transforms.Compose([
        # 1. Resizeæœ€çŸ­è¾¹åˆ°224ï¼Œä½¿ç”¨BICUBICæ’å€¼ï¼ˆä¸processorä¸€è‡´ï¼‰
        transforms.Resize(processor.size["shortest_edge"], interpolation=Resampling.BICUBIC),
        # 2. Center cropåˆ°224x224
        transforms.CenterCrop((processor.crop_size["height"], processor.crop_size["width"])),
        # 3. è½¬ä¸ºtensorå¹¶è‡ªåŠ¨é™¤ä»¥255
        transforms.ToTensor(),
        # 4. æ ‡å‡†åŒ–ï¼ˆä¸processorç›¸åŒå‚æ•°ï¼‰
        transforms.Normalize(
            mean=processor.image_mean,
            std=processor.image_std
        )
    ])
    
    return transform

if __name__ == '__main__':
    import numpy as np
    import time
    
    def verify_phikon2_preprocessing():
        """éªŒè¯ä¸¤ç§é¢„å¤„ç†æ–¹å¼æ˜¯å¦ç­‰ä»·"""
        
        # åˆå§‹åŒ–processor
        processor = AutoImageProcessor.from_pretrained("owkin/phikon-v2")
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        print("ğŸ” Phikon2 Processorä¿¡æ¯:")
        print(f"  Resizeå°ºå¯¸: {processor.size}")
        print(f"  Cropå°ºå¯¸: {processor.crop_size}")
        print(f"  å‡å€¼: {processor.image_mean}")
        print(f"  æ ‡å‡†å·®: {processor.image_std}")
        print(f"  Rescaleå› å­: {processor.rescale_factor}")
        print(f"  æ’å€¼æ–¹æ³•: {Resampling(processor.resample)} (å€¼={processor.resample})")
        
        # æ–¹æ³•1ï¼šå½“å‰æ–¹å¼ï¼ˆprocessoré¢„å¤„ç†ï¼‰
        def method1_current(pil_image):
            """å½“å‰æ–¹å¼ï¼šPIL -> processor -> tensor"""
            inputs = processor(pil_image, return_tensors="pt")
            return inputs["pixel_values"]
        
        # æ–¹æ³•2ï¼šä¿®å¤æ–¹å¼ï¼ˆtorchvisioné¢„å¤„ç†ï¼‰
        def method2_fixed(pil_image):
            """ä¿®å¤æ–¹å¼ï¼šPIL -> torchvision transforms -> tensor"""
            transform = get_phikon2_trans()
            tensor = transform(pil_image)
            return tensor.unsqueeze(0)  # æ·»åŠ batchç»´åº¦
        
        # åˆ›å»ºæµ‹è¯•å›¾åƒ
        test_image = Image.new('RGB', (256, 256), color=(128, 64, 192))
        
        print(f"\nğŸ§ª æµ‹è¯•å›¾åƒ: {test_image.size}, mode: {test_image.mode}")
        
        # æµ‹è¯•é¢„å¤„ç†ç­‰ä»·æ€§
        tensor1 = method1_current(test_image)
        tensor2 = method2_fixed(test_image)
        
        print(f"\nğŸ“Š é¢„å¤„ç†ç»“æœå¯¹æ¯”:")
        print(f"  Processoræ–¹å¼: shape={tensor1.shape}, dtype={tensor1.dtype}")
        print(f"  Transformæ–¹å¼: shape={tensor2.shape}, dtype={tensor2.dtype}")
        print(f"  æ•°å€¼èŒƒå›´: [{tensor1.min():.6f}, {tensor1.max():.6f}] vs [{tensor2.min():.6f}, {tensor2.max():.6f}]")
        
        # è®¡ç®—å·®å¼‚
        diff = torch.abs(tensor1 - tensor2)
        max_diff = diff.max().item()
        mean_diff = diff.mean().item()
        
        print(f"  æœ€å¤§å·®å¼‚: {max_diff:.10f}")
        print(f"  å¹³å‡å·®å¼‚: {mean_diff:.10f}")
        
        # éªŒè¯ç­‰ä»·æ€§
        tolerance = 1e-6
        if max_diff < tolerance:
            print(f"  âœ… é¢„å¤„ç†ç­‰ä»·æ€§éªŒè¯é€šè¿‡ï¼(å·®å¼‚ < {tolerance})")
        else:
            print(f"  âŒ é¢„å¤„ç†ç­‰ä»·æ€§éªŒè¯å¤±è´¥ï¼å·®å¼‚è¿‡å¤§: {max_diff}")
            
        # æ€§èƒ½æµ‹è¯•
        print(f"\nâš¡ æ€§èƒ½å¯¹æ¯”æµ‹è¯•:")
        
        # æµ‹è¯•processoræ–¹å¼
        start_time = time.time()
        for _ in range(100):
            _ = method1_current(test_image)
        time1 = time.time() - start_time
        
        # æµ‹è¯•transformæ–¹å¼  
        start_time = time.time()
        for _ in range(100):
            _ = method2_fixed(test_image)
        time2 = time.time() - start_time
        
        print(f"  Processoræ–¹å¼: {time1:.4f}s (100æ¬¡)")
        print(f"  Transformæ–¹å¼: {time2:.4f}s (100æ¬¡)")
        print(f"  é€Ÿåº¦æå‡: {time1/time2:.2f}x")
        
        return max_diff < tolerance
    
    # æ‰§è¡ŒéªŒè¯
    print("=" * 60)
    print("ğŸ§ª Phikon2 é¢„å¤„ç†ç­‰ä»·æ€§éªŒè¯")
    print("=" * 60)
    
    success = verify_phikon2_preprocessing()
    
    print(f"\nğŸ¯ éªŒè¯ç»“æœ: {'âœ… é€šè¿‡' if success else 'âŒ å¤±è´¥'}")
    if success:
        print("ğŸš€ Phikon2æ¨¡å‹å·²ä¼˜åŒ–ï¼Œé¢„æœŸGPUåˆ©ç”¨ç‡å°†æ˜¾è‘—æå‡ï¼")
    