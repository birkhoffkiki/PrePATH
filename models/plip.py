from PIL import Image
import requests
from transformers import CLIPProcessor, CLIPModel
import torch
from torchvision import transforms
from PIL.Image import Resampling

def get_plip_trans():
    """
    è·å–PLIPçš„é¢„å¤„ç†transformï¼Œå®Œå…¨ç­‰ä»·äºCLIPProcessor
    """
    processor = CLIPProcessor.from_pretrained('vinid/plip')
    img_proc = processor.image_processor
    
    transform = transforms.Compose([
        # 1. Resizeæœ€çŸ­è¾¹åˆ°224ï¼Œä½¿ç”¨BICUBICæ’å€¼
        transforms.Resize(img_proc.size["shortest_edge"], interpolation=Resampling.BICUBIC),
        # 2. Center cropåˆ°224x224
        transforms.CenterCrop((img_proc.crop_size["height"], img_proc.crop_size["width"])),
        # 3. è½¬ä¸ºtensorå¹¶è‡ªåŠ¨é™¤ä»¥255
        transforms.ToTensor(),
        # 4. æ ‡å‡†åŒ–
        transforms.Normalize(
            mean=img_proc.image_mean,
            std=img_proc.image_std
        )
    ])
    
    return transform

def print_data_info(inputs):
    tensor = inputs['pixel_values']
    print('Device:{}, shape:{}, max:{:.4f}, min: {:.4f}'.format(tensor.device,
            tensor.shape, tensor.max(), tensor.min()))


def plip(device, gpu_num):
    model = CLIPModel.from_pretrained('vinid/plip').to(device)

    pytorch_total_params = sum(p.numel() for p in model.vision_model.parameters())
    print(pytorch_total_params/1000/1000)

    model.eval()

    def func(image):
        # æ£€æŸ¥è¾“å…¥ç±»å‹ï¼Œæ”¯æŒä¸¤ç§æ–¹å¼
        if isinstance(image, torch.Tensor):
            # å·²ç»é¢„å¤„ç†è¿‡çš„tensorï¼ˆæ¥è‡ªDataLoaderï¼‰
            if image.dim() == 3:  # (3, 224, 224)
                image = image.unsqueeze(0)  # æ·»åŠ batchç»´åº¦
            inputs = {"pixel_values": image.to(device, non_blocking=True)}
        else:
            # PILå›¾åƒï¼ˆå…¼å®¹åŸæœ‰æ–¹å¼ï¼Œä½†æ•ˆç‡è¾ƒä½ï¼‰
            processor = CLIPProcessor.from_pretrained('vinid/plip')
            inputs = processor(images=image, return_tensors="pt")
            inputs = {k: v.to(device, non_blocking=True) for k, v in inputs.items()}
        
        # print_data_info(inputs)
        with torch.inference_mode():
            img_embed = model.get_image_features(**inputs)
        # img_embed = img_embed / img_embed.norm(dim=-1, keepdim=True)
        return img_embed
    return func


def plip_transformers():
    mean = (0.48145466, 0.4578275, 0.40821073)
    std = (0.26862954, 0.26130258, 0.27577711)
    trnsfrms_val = transforms.Compose(
        [
            transforms.Resize(224),
            transforms.ToTensor(),
            transforms.Normalize(mean = mean, std = std)
        ]
    )
    return trnsfrms_val

if __name__ == '__main__':
    import numpy as np
    from PIL import Image
    import time
    
    def verify_plip_preprocessing():
        """éªŒè¯ä¸¤ç§é¢„å¤„ç†æ–¹å¼æ˜¯å¦ç­‰ä»·"""
        
        # åˆå§‹åŒ–processor
        processor = CLIPProcessor.from_pretrained('vinid/plip')
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        print("ğŸ” PLIP Processorä¿¡æ¯:")
        img_proc = processor.image_processor
        print(f"  Resizeå°ºå¯¸: {img_proc.size}")
        print(f"  Cropå°ºå¯¸: {img_proc.crop_size}")
        print(f"  å‡å€¼: {img_proc.image_mean}")
        print(f"  æ ‡å‡†å·®: {img_proc.image_std}")
        from PIL.Image import Resampling
        print(f"  æ’å€¼æ–¹æ³•: {Resampling(img_proc.resample)} (å€¼={img_proc.resample})")
        
        # æ–¹æ³•1ï¼šå½“å‰æ–¹å¼ï¼ˆprocessoré¢„å¤„ç†ï¼‰
        def method1_current(pil_image):
            """å½“å‰æ–¹å¼ï¼šPIL -> processor -> tensor"""
            # ä½¿ç”¨imageså‚æ•°æ˜ç¡®æŒ‡å®šåªå¤„ç†å›¾åƒ
            inputs = processor(images=pil_image, return_tensors="pt")
            return inputs["pixel_values"]
        
        # æ–¹æ³•2ï¼šä¿®å¤æ–¹å¼ï¼ˆtorchvisioné¢„å¤„ç†ï¼‰
        def method2_fixed(pil_image):
            """ä¿®å¤æ–¹å¼ï¼šPIL -> torchvision transforms -> tensor"""
            transform = get_plip_trans()
            tensor = transform(pil_image)
            return tensor.unsqueeze(0)  # æ·»åŠ batchç»´åº¦
        
        # åˆ›å»ºæµ‹è¯•å›¾åƒ
        test_image = Image.new('RGB', (256, 256), color=(128, 64, 192))
        
        print(f"\nğŸ§ª æµ‹è¯•å›¾åƒ: {test_image.size}, mode: {test_image.mode}")
        
        # æµ‹è¯•é¢„å¤„ç†ç­‰ä»·æ€§
        tensor1 = method1_current(test_image)
        tensor2 = method2_fixed(test_image)
        
        print(f"\nğŸ“Š é¢„å¤„ç†ç»“æœå¯¹æ¯”:")
        print(f"  Processoræ–¹å¼: shape={tensor1.shape}, dtype={tensor1.dtype}")
        print(f"  Transformæ–¹å¼: shape={tensor2.shape}, dtype={tensor2.dtype}")
        print(f"  æ•°å€¼èŒƒå›´: [{tensor1.min():.6f}, {tensor1.max():.6f}] vs [{tensor2.min():.6f}, {tensor2.max():.6f}]")
        
        # è®¡ç®—å·®å¼‚
        diff = torch.abs(tensor1 - tensor2)
        max_diff = diff.max().item()
        mean_diff = diff.mean().item()
        
        print(f"  æœ€å¤§å·®å¼‚: {max_diff:.10f}")
        print(f"  å¹³å‡å·®å¼‚: {mean_diff:.10f}")
        
        # éªŒè¯ç­‰ä»·æ€§
        tolerance = 1e-6
        if max_diff < tolerance:
            print(f"  âœ… é¢„å¤„ç†ç­‰ä»·æ€§éªŒè¯é€šè¿‡ï¼(å·®å¼‚ < {tolerance})")
        else:
            print(f"  âŒ é¢„å¤„ç†ç­‰ä»·æ€§éªŒè¯å¤±è´¥ï¼å·®å¼‚è¿‡å¤§: {max_diff}")
            
        # æ€§èƒ½æµ‹è¯•
        print(f"\nâš¡ æ€§èƒ½å¯¹æ¯”æµ‹è¯•:")
        
        # æµ‹è¯•processoræ–¹å¼
        start_time = time.time()
        for _ in range(100):
            _ = method1_current(test_image)
        time1 = time.time() - start_time
        
        # æµ‹è¯•transformæ–¹å¼  
        start_time = time.time()
        for _ in range(100):
            _ = method2_fixed(test_image)
        time2 = time.time() - start_time
        
        print(f"  Processoræ–¹å¼: {time1:.4f}s (100æ¬¡)")
        print(f"  Transformæ–¹å¼: {time2:.4f}s (100æ¬¡)")
        print(f"  é€Ÿåº¦æå‡: {time1/time2:.2f}x")
        
        return max_diff < tolerance
    
    # æ‰§è¡ŒéªŒè¯
    print("=" * 60)
    print("ğŸ§ª PLIP é¢„å¤„ç†ç­‰ä»·æ€§éªŒè¯")
    print("=" * 60)
    
    success = verify_plip_preprocessing()
    
    print(f"\nğŸ¯ éªŒè¯ç»“æœ: {'âœ… é€šè¿‡' if success else 'âŒ å¤±è´¥'}")
    if success:
        print("ğŸš€ PLIPæ¨¡å‹å·²ä¼˜åŒ–ï¼Œé¢„æœŸGPUåˆ©ç”¨ç‡å°†æ˜¾è‘—æå‡ï¼")